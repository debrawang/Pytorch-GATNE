{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "from walk import RWGraph\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from gensim.models.keyedvectors import Vocab\n",
    "from numpy import random\n",
    "from six import iteritems\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (auc, f1_score, precision_recall_curve, roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#路径设定\n",
    "file_name = '../Tdata'\n",
    "#超参数设定\n",
    "embedding_size = 200\n",
    "embedding_u_size = 10\n",
    "num_sampled = 5 #负采样个数\n",
    "dim_a = 20 #attention维度\n",
    "att_head = 1\n",
    "neighbor_samples = 5\n",
    "schema = None # metapath的制定schema\n",
    "num_walks = 20 #路径数\n",
    "walk_length = 10 #\n",
    "window_size = 5 #上下文窗口大小,应小于walk_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据读取\n",
    "def load_training_data(f_name):\n",
    "    print('We are loading data from:', f_name)\n",
    "    edge_data_by_type = dict()\n",
    "    all_edges = list()\n",
    "    all_nodes = list()\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line[:-1].split(' ')\n",
    "            if words[0] not in edge_data_by_type:\n",
    "                edge_data_by_type[words[0]] = list()\n",
    "            x, y = words[1], words[2]\n",
    "            edge_data_by_type[words[0]].append((x, y))\n",
    "            all_edges.append((x, y))\n",
    "            all_nodes.append(x)\n",
    "            all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))\n",
    "    all_edges = list(set(all_edges))\n",
    "    edge_data_by_type['Base'] = all_edges\n",
    "    print('Total training nodes: ' + str(len(all_nodes)))\n",
    "    return edge_data_by_type\n",
    "def load_testing_data(f_name):\n",
    "    print('We are loading data from:', f_name)\n",
    "    true_edge_data_by_type = dict()\n",
    "    false_edge_data_by_type = dict()\n",
    "    all_edges = list()\n",
    "    all_nodes = list()\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line[:-1].split(' ')\n",
    "            x, y = words[1], words[2]\n",
    "            if int(words[3]) == 1:\n",
    "                if words[0] not in true_edge_data_by_type:\n",
    "                    true_edge_data_by_type[words[0]] = list()\n",
    "                true_edge_data_by_type[words[0]].append((x, y))\n",
    "            else:\n",
    "                if words[0] not in false_edge_data_by_type:\n",
    "                    false_edge_data_by_type[words[0]] = list()\n",
    "                false_edge_data_by_type[words[0]].append((x, y))\n",
    "            all_nodes.append(x)\n",
    "            all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))\n",
    "    return true_edge_data_by_type, false_edge_data_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are loading data from: ../Tdata/train.txt\n",
      "Total training nodes: 511\n",
      "We are loading data from: ../Tdata/valid.txt\n",
      "We are loading data from: ../Tdata/test.txt\n"
     ]
    }
   ],
   "source": [
    "network_data = load_training_data(file_name + '/train.txt')\n",
    "valid_true_data_by_edge, valid_false_data_by_edge = load_testing_data(file_name + '/valid.txt')\n",
    "testing_true_data_by_edge, testing_false_data_by_edge = load_testing_data(file_name + '/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#路径生成\n",
    "#从edgse生成Graph\n",
    "def get_G_from_edges(edges):\n",
    "    edge_dict = dict()\n",
    "    for edge in edges:\n",
    "        edge_key = str(edge[0]) + '_' + str(edge[1])\n",
    "        if edge_key not in edge_dict:\n",
    "            edge_dict[edge_key] = 1\n",
    "        else:\n",
    "            edge_dict[edge_key] += 1\n",
    "    tmp_G = nx.Graph()\n",
    "    for edge_key in edge_dict:\n",
    "        weight = edge_dict[edge_key]\n",
    "        x = edge_key.split('_')[0]\n",
    "        y = edge_key.split('_')[1]\n",
    "        tmp_G.add_edge(x, y)\n",
    "        tmp_G[x][y]['weight'] = weight\n",
    "    return tmp_G\n",
    "\n",
    "def generate_walks(network_data):\n",
    "    base_network = network_data['Base']\n",
    "    if schema is not None:\n",
    "        node_type = load_node_type('../data/node_type.txt')\n",
    "    else:\n",
    "        node_type = None\n",
    "        \n",
    "    base_walker = RWGraph(get_G_from_edges(base_network), node_type=node_type)\n",
    "    base_walks = base_walker.simulate_walks(num_walks, walk_length, schema=schema)\n",
    "    all_walks = []\n",
    "    for layer_id in network_data:\n",
    "        if layer_id == 'Base':\n",
    "            continue\n",
    "\n",
    "        tmp_data = network_data[layer_id]\n",
    "        # start to do the random walk on a layer\n",
    "\n",
    "        layer_walker = RWGraph(get_G_from_edges(tmp_data))\n",
    "        layer_walks = layer_walker.simulate_walks(num_walks, walk_length)\n",
    "\n",
    "        all_walks.append(layer_walks)\n",
    "\n",
    "    print('路径生成完毕！')\n",
    "\n",
    "    return base_walks, all_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路径生成完毕！\n"
     ]
    }
   ],
   "source": [
    "base_walks, all_walks = generate_walks(network_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成index2word\n",
    "def generate_vocab(all_walks):\n",
    "    index2word = []\n",
    "    raw_vocab = defaultdict(int)\n",
    "\n",
    "    for walks in all_walks:\n",
    "        for walk in walks:\n",
    "            for word in walk:\n",
    "                raw_vocab[word] += 1\n",
    "\n",
    "    vocab = {}\n",
    "    for word, v in iteritems(raw_vocab):\n",
    "        vocab[word] = Vocab(count=v, index=len(index2word))\n",
    "        index2word.append(word)\n",
    "\n",
    "    index2word.sort(key=lambda word: vocab[word].count, reverse=True)\n",
    "    for i, word in enumerate(index2word):\n",
    "        vocab[word].index = i\n",
    "    \n",
    "    return vocab, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, index2word = generate_vocab([base_walks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name+'/feature.txt', 'r') as f:\n",
    "    first = True\n",
    "    for line in f:\n",
    "        if first:\n",
    "            items_first = line.strip().split() \n",
    "            feature_dic = np.zeros([len(vocab),int(items_first[1])])            \n",
    "            first = False\n",
    "            continue\n",
    "        items = line.strip().split()\n",
    "        if items[0] in vocab:\n",
    "            feature_dic[vocab[items[0]].index] = np.array(items[1:],dtype=float)\n",
    "feature_dic = torch.Tensor(feature_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(all_walks, vocab):\n",
    "    pairs = []\n",
    "    skip_window = window_size // 2\n",
    "    for layer_id, walks in enumerate(all_walks):\n",
    "        for walk in walks:\n",
    "            for i in range(len(walk)):\n",
    "                for j in range(1, skip_window + 1):\n",
    "                    if i - j >= 0:\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i - j]].index, layer_id))\n",
    "                    if i + j < len(walk):\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i + j]].index, layer_id))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = generate_pairs(all_walks, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确保Base边在最后一个\n",
    "edge_types = list(network_data.keys())\n",
    "if edge_types[-1] != 'Base':\n",
    "    edge_types.sort()\n",
    "    edge_types.remove('Base')\n",
    "    edge_types.append('Base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络参数计算\n",
    "num_nodes = len(index2word)\n",
    "edge_type_count = len(edge_types) - 1\n",
    "u_num = edge_type_count\n",
    "neighbors = [[[] for __ in range(edge_type_count)] for _ in range(num_nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据neighbor_samples去截断、填充邻居，如果一个实体点没有邻居，则认为它的邻居是他自己\n",
    "for r in range(edge_type_count):\n",
    "    g = network_data[edge_types[r]]\n",
    "    for (x, y) in g:\n",
    "        ix = vocab[x].index\n",
    "        iy = vocab[y].index\n",
    "        neighbors[ix][r].append(iy)\n",
    "        neighbors[iy][r].append(ix)\n",
    "    for i in range(num_nodes):\n",
    "        if len(neighbors[i][r]) == 0:\n",
    "            neighbors[i][r] = [i] * neighbor_samples\n",
    "        elif len(neighbors[i][r]) < neighbor_samples:\n",
    "            neighbors[i][r].extend(list(np.random.choice(neighbors[i][r], size=neighbor_samples-len(neighbors[i][r]))))\n",
    "        elif len(neighbors[i][r]) > neighbor_samples:\n",
    "            neighbors[i][r] = list(np.random.choice(neighbors[i][r], size=neighbor_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行负采样\n",
    "def negative_sampling(targets, vocab, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].item()\n",
    "        while len(nsample) < k: # num of sampling\n",
    "            neg = random.choice(len(vocab))\n",
    "            if neg == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(np.array(nsample)) \n",
    "    return np.array(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, true_edges, false_edges):\n",
    "\n",
    "    true_list = list()\n",
    "    prediction_list = list()\n",
    "    true_num = 0\n",
    "    for edge in true_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        \n",
    "        if tmp_score is not None:\n",
    "            true_list.append(1)\n",
    "            prediction_list.append(tmp_score)\n",
    "            true_num += 1\n",
    "    for edge in false_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        \n",
    "        if tmp_score is not None:\n",
    "            true_list.append(0)\n",
    "            prediction_list.append(tmp_score)\n",
    "\n",
    "    sorted_pred = prediction_list[:]\n",
    "    sorted_pred.sort()\n",
    "    threshold = sorted_pred[-true_num]\n",
    "    y_pred = np.zeros(len(prediction_list), dtype=np.int32)\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] >= threshold:\n",
    "            y_pred[i] = 1\n",
    "    y_true = np.array(true_list)\n",
    "    y_scores = np.array(prediction_list)\n",
    "    ps, rs, _ = precision_recall_curve(y_true, y_scores)\n",
    "    return roc_auc_score(y_true, y_scores), f1_score(y_true, y_pred), auc(rs, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(local_model, node1, node2):\n",
    "\n",
    "    try:\n",
    "        vector1 = np.array(local_model[node1].view(-1).tolist())\n",
    "        vector2 = np.array(local_model[node2].view(-1).tolist())\n",
    "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(pairs, neighbors, batch_size):\n",
    "    n_batches = (len(pairs) + (batch_size - 1)) // batch_size\n",
    "    for idx in range(n_batches):\n",
    "        x, y, t, neigh = [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            index = idx * batch_size + i\n",
    "            if index >= len(pairs):\n",
    "                break\n",
    "            x.append(pairs[index][0])\n",
    "            y.append(pairs[index][1])\n",
    "            t.append(pairs[index][2])\n",
    "            neigh.append(neighbors[pairs[index][0]])\n",
    "        yield (np.array(x).astype(np.int32), np.array(y).reshape(-1, 1).astype(np.int32), np.array(t).astype(np.int32), np.array(neigh).astype(np.int32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构,核心代码\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_size, u_num, dim_a, att_head, edge_type_count, feature_dic):\n",
    "        super(MyLayer, self).__init__()\n",
    "        # attention参数传递\n",
    "        self.u_num = u_num\n",
    "        self.att_head = att_head\n",
    "        self.embedding_size = embedding_size\n",
    "        feature_dim = feature_dic.shape[1]\n",
    "        self.edge_type_count = edge_type_count\n",
    "        self.embedding_u_size = embedding_u_size\n",
    "        # 初始化层\n",
    "        self.feature_dic = feature_dic # Tensor(node_num*feature_dim)\n",
    "        self.feature_layer1 = nn.Linear(feature_dim, embedding_size)\n",
    "        self.feature_layer2 = nn.Linear(feature_dim, embedding_size)\n",
    "        self.neigh_feature_trans = nn.Parameter(torch.Tensor(edge_type_count, feature_dim, embedding_u_size))\n",
    "        self.neigh_linear_1 = nn.Linear(embedding_u_size, dim_a)\n",
    "        self.neigh_linear_2 = nn.Linear(dim_a, att_head)\n",
    "        self.neigh_linear_last = nn.Linear(embedding_u_size, embedding_size // att_head)\n",
    "    def forward(self,input_node,node_neigh):\n",
    "        # input_node:LongTensor(1),label_node:LongTensor(1)\n",
    "        # node_neigh:LongTensor(edge_type_num*neigh_sample)\n",
    "        batch_size = max(1,input_node.shape[0])\n",
    "        node_feature = torch.index_select(self.feature_dic, 0, input_node) # 提取输入node的特征\n",
    "        node_embed = self.feature_layer1(node_feature)\n",
    "        node_weight = self.feature_layer2 (node_feature)\n",
    "        node_neigh = torch.unbind(node_neigh, dim=1)\n",
    "        neigh_type_embedding = torch.empty([batch_size,self.edge_type_count,self.embedding_u_size]).cuda()\n",
    "        for j in range(batch_size):\n",
    "            neigh_feature_temp = torch.cat([torch.matmul(torch.index_select(self.feature_dic, 0, node_neigh[i][j]), self.neigh_feature_trans[i]) for i in range(self.edge_type_count)])\n",
    "            neigh_type_embedding[j] = torch.mean(neigh_feature_temp,0)\n",
    "        # attention层\n",
    "        attention = nn.Tanh()(self.neigh_linear_1(neigh_type_embedding))\n",
    "        attention = self.neigh_linear_2(attention)\n",
    "        attention = nn.Softmax()(attention.view(-1,self.u_num))\n",
    "        attention = attention.view(-1, self.att_head, self.u_num)\n",
    "        node_type_embed = torch.matmul(attention,neigh_type_embedding)\n",
    "        node_embed = node_embed + self.neigh_linear_last(node_type_embed).view(-1,self.embedding_size) + node_weight\n",
    "        last_node_embed = nn.functional.normalize(node_embed, p=2, dim=1, eps=1e-12, out=None)\n",
    "        return last_node_embed.reshape(batch_size,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self,num_nodes, embedding_size, u_num, dim_a, att_head, edge_type_count, feature_dic):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.Embedding_layer = MyLayer(num_nodes, embedding_size, u_num, dim_a, att_head, edge_type_count, feature_dic)\n",
    "        self.embedding_u = nn.Embedding(num_nodes, embedding_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "        # 初始化out_embedding\n",
    "        self.embedding_u.weight.data.uniform_(-0.0, 0.0)\n",
    "    def forward(self, input_node, node_neigh, label_node, neg_node):\n",
    "        center_embeds = self.Embedding_layer(input_node,node_neigh)\n",
    "        target_embeds = self.embedding_u(label_node)\n",
    "        \n",
    "        neg_embeds = -self.embedding_u(neg_node)\n",
    "        \n",
    "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1\n",
    "        negative_score = torch.sum(neg_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2), 1).view(neg_node.size(0), -1) # BxK -> Bx1\n",
    "        \n",
    "        loss = self.logsigmoid(positive_score) + self.logsigmoid(negative_score)\n",
    "        \n",
    "        return -torch.mean(loss)\n",
    "    def prediction(self,input_node,node_neigh):\n",
    "        return self.Embedding_layer(input_node,node_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练参数\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 200\n",
    "NEG = 5\n",
    "set_patience = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = MyNet(num_nodes, embedding_size, u_num, dim_a, att_head, edge_type_count, feature_dic.cuda()).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "g_iter = 0\n",
    "best_score = 0\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 1.02\n",
      "valid auc: 0.7605699985380243\n",
      "valid pr: 0.6912321270006594\n",
      "valid f1: 0.6747067448680352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid auc: 0.7629230054781091\n",
      "valid pr: 0.6798681835336058\n",
      "valid f1: 0.7005131964809383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid auc: 0.7782880049191184\n",
      "valid pr: 0.7006228011934703\n",
      "valid f1: 0.7005131964809383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid auc: 0.7887299300831607\n",
      "valid pr: 0.7165362006740535\n",
      "valid f1: 0.7086510263929617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid auc: 0.79909160782931\n",
      "valid pr: 0.7453846218545253\n",
      "valid f1: 0.7005131964809383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid auc: 0.8110246837402499\n",
      "valid pr: 0.7551125917991836\n",
      "valid f1: 0.7005131964809383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid auc: 0.816698977046981\n",
      "valid pr: 0.765804133477759\n",
      "valid f1: 0.7151026392961877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid auc: 0.8172335549229883\n",
      "valid pr: 0.7659148836018382\n",
      "valid f1: 0.7232404692082112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid auc: 0.8139292640242171\n",
      "valid pr: 0.7660570557245747\n",
      "valid f1: 0.7329178885630498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid auc: 0.811097879275204\n",
      "valid pr: 0.7610934838277361\n",
      "valid f1: 0.729692082111437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, mean_loss : 0.61\n",
      "valid auc: 0.8119275827521263\n",
      "valid pr: 0.7638558736939722\n",
      "valid f1: 0.729692082111437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid auc: 0.8075977588772028\n",
      "valid pr: 0.7629593023920498\n",
      "valid f1: 0.729692082111437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid auc: 0.8112711341491732\n",
      "valid pr: 0.7663238331874105\n",
      "valid f1: 0.729692082111437\n",
      "Early Stopping\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i,batch in enumerate(getBatch(train_pairs,neighbors,BATCH_SIZE)):\n",
    "\n",
    "        input_node = torch.LongTensor(batch[0]).cuda()\n",
    "        label_node = torch.LongTensor(batch[1]).cuda()\n",
    "        node_neigh = torch.LongTensor(batch[3]).cuda()\n",
    "        neg_node = torch.LongTensor(negative_sampling(label_node, vocab, NEG)).cuda()\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = model(input_node, node_neigh, label_node, neg_node)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "        losses.append(loss.data.tolist())\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch, np.mean(losses)))\n",
    "        losses = []\n",
    "    final_model = dict(zip(edge_types[:-1], [dict() for _ in range(edge_type_count)]))\n",
    "    for i in range(edge_type_count):\n",
    "        for j in range(num_nodes):\n",
    "            input_node = torch.LongTensor([j]).cuda()\n",
    "            node_neigh = torch.LongTensor(neighbors[j]).cuda()\n",
    "            final_model[edge_types[i]][index2word[j]] = model.prediction(input_node,node_neigh)\n",
    "            valid_aucs, valid_f1s, valid_prs = [], [], []\n",
    "            test_aucs, test_f1s, test_prs = [], [], []\n",
    "    for i in range(edge_type_count):\n",
    "        tmp_auc, tmp_f1, tmp_pr = evaluate(final_model[edge_types[i]], valid_true_data_by_edge[edge_types[i]], valid_false_data_by_edge[edge_types[i]])\n",
    "        valid_aucs.append(tmp_auc)\n",
    "        valid_f1s.append(tmp_f1)\n",
    "        valid_prs.append(tmp_pr)\n",
    "\n",
    "        tmp_auc, tmp_f1, tmp_pr = evaluate(final_model[edge_types[i]], testing_true_data_by_edge[edge_types[i]], testing_false_data_by_edge[edge_types[i]])\n",
    "        test_aucs.append(tmp_auc)\n",
    "        test_f1s.append(tmp_f1)\n",
    "        test_prs.append(tmp_pr)\n",
    "    print('valid auc:', np.mean(valid_aucs))\n",
    "    print('valid pr:', np.mean(valid_prs))\n",
    "    print('valid f1:', np.mean(valid_f1s))\n",
    "\n",
    "    average_auc = np.mean(test_aucs)\n",
    "    average_f1 = np.mean(test_f1s)\n",
    "    average_pr = np.mean(test_prs)\n",
    "\n",
    "    cur_score = np.mean(valid_aucs)\n",
    "    if cur_score > best_score:\n",
    "        best_score = cur_score\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience > set_patience:            \n",
    "            output_name = '../Tdata/result_pytorch.txt'\n",
    "            f = open(output_name,'w+')\n",
    "            f.write('Overall ROC-AUC:' + str(average_auc) + '\\n')\n",
    "            f.write('Overall PR-AUC'+ str(average_pr) + '\\n')\n",
    "            f.write('Overall F1:'+ str(average_f1) + '\\n') \n",
    "            print('Early Stopping')\n",
    "            break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“wld_pytorch”",
   "language": "python",
   "name": "wld_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
